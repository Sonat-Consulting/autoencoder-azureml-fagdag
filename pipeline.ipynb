{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Azure ML SDK V2"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nå som Azure ML SDK V2 skal ut av preview, kan det være greit å få testet ut hvordan man oppretter gjenbrukbare komponenter. Som nevnt tidligere, merk at det er et par andre måter dette kan gjøres på med V2, som ikke er releaset i skrivende stund. Så dagens approach blir en script-yaml combo, der scriptet inneholder logikken for hva komponenten din skal gjøre, mens YAML filen definerer hva komponenten er(altså, hva den tar inn, gir ut, hva den trenger for å kjøre etc...). Om du sitter fast, gjerne spør en av oss som holder fagdagen, snakk med sidemannen, eller referer til denne \"tutorialen\" fra Microsoft om du vil ha det litt lettere https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipelines-ui . For ordens skyld har vi satt det opp slik at det genereres en filstruktur, og du kan jobbe i notebooken hele tiden. Målet med denne notebooken er å få inn arbeidet du gjorde i del 1 av denne fagdagen inn i Azure Machine Learning Studio som en pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Her har dere et eksempel å gå etter. Denne prep.py filen henter inn argumenter via parser filen. Her er det viktig at det stemmer overens med det dere definerer i yaml filen, da det er det som forteller azure hva som kommer inn og ut av komponenten. Her er prep steget der vi henter inn mnist datasettet, gjør det om fra dataframe til numpy(grunnet mltable kun har to_pandas_dataframe()), og prosesserer det. Så lagrer vi de som npy filer(binaries) på lokasjonen som er definert i output_test(Denne vil bli definert automatisk når du kobler opp pipelinen)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "prep_src_dir = \"./components/prep\"\r\n",
        "os.makedirs(prep_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666727301390
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {prep_src_dir}/prep.py\r\n",
        "\r\n",
        "import argparse\r\n",
        "import mltable\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "parser = argparse.ArgumentParser(\"prep\")\r\n",
        "parser.add_argument(\"--dataset\", type=str, help=\"The mltable training data\")\r\n",
        "parser.add_argument('--output_train', type=str, help='Path of train set')\r\n",
        "parser.add_argument('--output_test', type=str, help='Path of test set')\r\n",
        "\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "\r\n",
        "tbl_train = mltable.load(args.dataset)\r\n",
        "df = tbl_train.to_pandas_dataframe()\r\n",
        "\r\n",
        "X_train_flattened = df.drop(\"label\", axis=1).to_numpy().astype('uint8')\r\n",
        "X_train = X_train_flattened.reshape(X_train_flattened.shape[0], 28, 28)\r\n",
        "y_train = df[\"label\"].to_numpy().astype('uint8')\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\r\n",
        "\r\n",
        "\r\n",
        "# Filter training data \r\n",
        "train_filter = np.where(y_train == 2)\r\n",
        "test_filter = np.where(y_test == 2)\r\n",
        "\r\n",
        "X_train, Y_train = X_train[train_filter], y_train[train_filter]\r\n",
        "X_test, Y_test = X_test[test_filter], y_test[test_filter]\r\n",
        "\r\n",
        "X_train = X_train.reshape((X_train.shape[0], 28*28)).astype('float32')\r\n",
        "X_test = X_test.reshape((X_test.shape[0], 28*28)).astype('float32')\r\n",
        "X_train = X_train / 255\r\n",
        "X_test = X_test / 255\r\n",
        "\r\n",
        "print(X_train)\r\n",
        "print(X_test)\r\n",
        "\r\n",
        "np.save(args.output_train + \"/train.npy\",  X_train)\r\n",
        "np.save(args.output_test + \"/test.npy\", X_test)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/prep/prep.py\n"
        }
      ],
      "execution_count": 96,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For å forsikre oss om at komponentene kjører, så trenger vi en conda.yml fil der du definerer dependencies. Her kan du lage en for hver komponent, eller en stor som skal håndtere alle komponentene. For mindre prosjekter skal dette gå fint, men med mange avhengigheter, så risikerer man fort at det tar lang tid å bygge images, også om du kun har lagt til en dependency for en spesifikk komponent, da det vil trigge nytt bygg for alle."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile components/conda.yml\r\n",
        "name: demo_env\r\n",
        "channels:\r\n",
        "  - conda-forge\r\n",
        "dependencies:\r\n",
        "  - python=3.7.6\r\n",
        "  - pip\r\n",
        "  - pip:\r\n",
        "    - pandas\r\n",
        "    - pyjokes\r\n",
        "    - mltable\r\n",
        "    - argparse\r\n",
        "    - azureml-dataprep[pandas]\r\n",
        "    - tensorflow\r\n",
        "    - scikit-learn\r\n",
        "    - numpy\r\n",
        "    - azureml-mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nå som prep scriptet ditt er klart, trenger du å definere en prep.yaml fil som er ment til å fortelle Azure hvordan komponenten din ser ut. Her er YAML definisjonen for prep komponenten"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./components/prep.yml\r\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\r\n",
        "name: prep_INSERT_NAME_HERE\r\n",
        "# version: 1\r\n",
        "display_name: Preprocessing step for mnist autoencoder\r\n",
        "type: command\r\n",
        "inputs:\r\n",
        "  dataset:\r\n",
        "    type: mltable\r\n",
        "outputs:\r\n",
        "  output_train:\r\n",
        "    type: uri_folder\r\n",
        "  output_test:\r\n",
        "    type: uri_folder\r\n",
        "environment:\r\n",
        "  conda_file: ./conda.yml\r\n",
        "  image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220314.v1\r\n",
        "code: ./prep\r\n",
        "command: >-\r\n",
        "  python prep.py \r\n",
        "  --dataset ${{inputs.dataset}} \r\n",
        "  --output_train ${{outputs.output_train}}\r\n",
        "  --output_test ${{outputs.output_test}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/prep.yml\n"
        }
      ],
      "execution_count": 69,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oppgave 1\r\n",
        "Her ønsker vi at dere lager en komponent med **i hvert fall** en input som da vil være treningsdata fra forrige steg. Du kan strukturere resten selv ut ifra hva du vil kunne gi inn til komponenten. Målet med komponenten er å trene en modell, og bruke denne i neste komponent."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "train_src_dir = \"./components/train\"\r\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 70,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666736353003
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lag et treningsskript på lignende måte som prep komponenten. Når du har \"fungerende\" logikk, legg til **%%writefile {train_src_dir}/train.py\r\n",
        "** på toppen av skriptet slik at det skrives til treningsmappen(Om du ikke allerede har definert det i .py filer istedenfor notebook). Her står du fritt til hvordan du vil lagre modellen, men vi anbefaler å bruke keras sin save_model (siden dette er en custom modell, så vil du helst bare lagre filen uten extension, da dette vil lage en tensorflow fil som er lettere å laste inn enn f.eks .h5 med mindre du har skrevet kode for det), du kan også bruke [mlflow](https://mlflow.org/) om du vil, men dette vil komme i en senere oppgave."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/train/train.py\n"
        }
      ],
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definer nå en YAML fil på lignende måte som preprosesseringen"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile components/train.yml\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting components/train.yml\n"
        }
      ],
      "execution_count": 84,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oppgave 3\r\n",
        "Nå som vi har en treningskomponent, har vi lyst til å score modellen. Avhengig av hvordan du lagret modellen. vil du bruke samme rammeverk til å laste den inn."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "score_src_dir = \"./components/score\"\r\n",
        "os.makedirs(score_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 85,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666798173496
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_src_dir}/score.py\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/score/score.py\n"
        }
      ],
      "execution_count": 86,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665477035988
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./components/score.yml\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/score.yml\n"
        }
      ],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oppgave 4\r\n",
        "Nå når du har dine tre komponenter er det klart for å faktisk deploye de. Først må du åpne en terminal og logge deg inn i azure. Prøv først az login --identity om du har en identitet storet, eller om du gjør dette via azure, ellers vil az login ta deg gjennom autentiseringsløypen.\r\n",
        "\r\n",
        "Når dette er gjort kan du endelig lage komponenter. Kjør kommandoen **az ml component create --file \"Path to yaml file\" ** for hver fil, og trykk deg videre til Designer i ML studio. Her velger du custom under new pipeline, og slå deg løs!\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oppgave 5\r\n",
        "Nå har du en fungerende pipeline, men biten med modellen kan ha vært litt kronglete. Nå skal vi ta i bruk [mlflow](https://mlflow.org/) biblioteket for lagring, lasting og logging av modellen."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/train/train.py\n"
        }
      ],
      "execution_count": 95,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_src_dir}/score.py\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/score/score.py\n"
        }
      ],
      "execution_count": 94,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Veien videre\r\n",
        "Å definere gjenbrukbare komponenter på denne måten har sine gode og dårlige sider. Noen ønsker gjerne å slippe filer og terminalkommandoer, mens andre foretrekker oppdeligen, da det naturlig kan falle inn i f.eks en CI/CD pipeline. Men for ordens skyld tenkte vi bare å kjapt trekke frem hovedpunktene av en annen variasjon, nemlig å definere hele pipelinen, istedenfor kun komponentene."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\r\n",
        "import os\r\n",
        "from pathlib import Path\r\n",
        "from mldesigner import command_component, Input, Output\r\n",
        "\r\n",
        "\r\n",
        "@command_component(\r\n",
        "    name=\"component_name\",\r\n",
        "    version=\"1\",\r\n",
        "    display_name=\"Display name\",\r\n",
        "    description=\"Insert description here\",\r\n",
        "    environment=dict(\r\n",
        "        conda_file=Path(__file__).parent / \"conda.yaml\",\r\n",
        "        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\r\n",
        "    ),\r\n",
        ")\r\n",
        "def component(\r\n",
        "    input_1: Input(type=\"<Insert Type>\"),\r\n",
        "    output_1: Output(type=\"<Insert Type>\"),\r\n",
        "    output_2: Output(type=\"<Insert Type>\"),\r\n",
        "):\r\n",
        "    do something\r\n",
        "```\r\n",
        "\r\n",
        "Her tar vi i bruk @command_component for å definere denne som en komponent. Merk at vi fremdeles trenger en conda.yaml fil for å definere hva denne komponenten trenger for å kunne kjøre, men du slipper nå å definere selve komponenten.\r\n",
        "Kunne alternativt blitt gjort uten decoratoren og med et eget python scrpit du refererer til, ved å lage en vanlig command, og referere til scriptet ditt. Merk at her slipper du også YAML definisjon, da du vil bruke én enkelt conda.yaml for selve environmentet du kjører i, istedenfor å legge dette ved på hver komponent\r\n",
        "\r\n",
        "```\r\n",
        "from azure.ai.ml import command\r\n",
        "from azure.ai.ml import Input, Output\r\n",
        "\r\n",
        "train_model_component = command(\r\n",
        "    name=\"train\",\r\n",
        "    display_name=\"Train\",\r\n",
        "    inputs={\r\n",
        "        \"training_data\": Input(type = \"uri_folder\"),\r\n",
        "        \"max_epocs\": Input(type = \"integer\", min = 0, max= 100),\r\n",
        "        \"learning_rate\": Input(type = \"number\", default= 0.01) \r\n",
        "    },\r\n",
        "    outputs={\r\n",
        "    \"model_output\": Output(type = \"uri_folder\")\r\n",
        "    },\r\n",
        "    # The source folder of the component\r\n",
        "    code=train_src_dir,\r\n",
        "    command=\"\"\"python train.py \\\r\n",
        "  --training_data ${{inputs.training_data}} --max_epocs ${{inputs.max_epocs}}   --learning_rate ${{inputs.learning_rate}}  \\\r\n",
        "  --model_output ${{outputs.model_output}}\r\n",
        "            \"\"\",\r\n",
        "    environment=\"azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",\r\n",
        ")\r\n",
        "```\r\n",
        "\r\n",
        "```\r\n",
        "# define a pipeline \r\n",
        "@pipeline(\r\n",
        "    default_compute=cpu_compute_target,\r\n",
        ")\r\n",
        "def pipeline(pipeline_input_data):\r\n",
        "    prepare_data_node = prepare_data_component(input_data=pipeline_input_data)\r\n",
        "\r\n",
        "    node1 = component(\r\n",
        "        input_data=prepare_data_node.outputs.training_data\r\n",
        "    )\r\n",
        "    node1.compute = gpu_compute_target\r\n",
        "\r\n",
        "    etc\r\n",
        "\r\n",
        "# create a pipeline\r\n",
        "pipeline_job = pipeline(pipeline_input_data=<dataset>)\r\n",
        "```\r\n",
        "\r\n",
        "Bruker også her @pipeline for å definere at dette er en pipeline. Om du er mer interessert i å videre utforske dette, sjekk her: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python (merk at den ble skrevet samme dag som denne notebooken, så du kan risikere at ting som mldesigner ikke funker...)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}